{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distinguishing comparison in sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data_extraction import ExtractMiddlePart, ExtractRawSentence\n",
    "from infersent.infersent_feature import initialize_infersent, InfersentFeature\n",
    "from elmo.elmo_feature import initialize_elmo, ElmoFeature\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/data.csv\")\n",
    "test = pd.read_csv(\"data/held-out-data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW + XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = make_pipeline(ExtractRawSentence(), CountVectorizer(), XGBClassifier(n_jobs=-1, n_estimators=1000))\n",
    "fitted = pl.fit(train, train['most_frequent_label'].values)\n",
    "predicted = fitted.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BETTER      0.643     0.553     0.594       273\n",
      "       WORSE      0.491     0.235     0.318       119\n",
      "        NONE      0.839     0.919     0.877      1048\n",
      "\n",
      "    accuracy                          0.793      1440\n",
      "   macro avg      0.658     0.569     0.597      1440\n",
      "weighted avg      0.773     0.793     0.777      1440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test['most_frequent_label'].values, predicted, labels=['BETTER', 'WORSE', 'NONE'], digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Middle part of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = make_pipeline(ExtractMiddlePart(), CountVectorizer(), XGBClassifier(n_jobs=-1, n_estimators=1000))\n",
    "fitted = pl.fit(train, train['most_frequent_label'].values)\n",
    "predicted = fitted.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BETTER      0.765     0.751     0.758       273\n",
      "       WORSE      0.542     0.328     0.408       119\n",
      "        NONE      0.903     0.948     0.925      1048\n",
      "\n",
      "    accuracy                          0.859      1440\n",
      "   macro avg      0.736     0.675     0.697      1440\n",
      "weighted avg      0.847     0.859     0.850      1440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test['most_frequent_label'].values, predicted, labels=['BETTER', 'WORSE', 'NONE'], digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InferSent + XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sentences = ExtractRawSentence().transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/anaconda3/lib/python3.7/site-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'models.BLSTMEncoder' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.rnn.LSTM' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded\n",
      "Found 14943(/16494) words with glove vectors\n",
      "Vocab size : 14943\n"
     ]
    }
   ],
   "source": [
    "infersent = initialize_infersent(full_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/Jupyter/ML/Final_project/models.py:206: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  sentences[stidx:stidx + bsize]), volatile=True)\n"
     ]
    }
   ],
   "source": [
    "pl = make_pipeline(ExtractRawSentence(), InfersentFeature(infersent), XGBClassifier(n_jobs=-1, n_estimators=1000))\n",
    "fitted = pl.fit(train, train['most_frequent_label'].values)\n",
    "predicted = fitted.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BETTER      0.708     0.586     0.641       273\n",
      "       WORSE      0.556     0.210     0.305       119\n",
      "        NONE      0.849     0.947     0.895      1048\n",
      "\n",
      "    accuracy                          0.817      1440\n",
      "   macro avg      0.704     0.581     0.614      1440\n",
      "weighted avg      0.798     0.817     0.798      1440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test['most_frequent_label'].values, predicted, labels=['BETTER', 'WORSE', 'NONE'], digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Middle part of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_part = ExtractMiddlePart().transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded\n",
      "Found 6021(/6555) words with glove vectors\n",
      "Vocab size : 6021\n"
     ]
    }
   ],
   "source": [
    "infersent = initialize_infersent(middle_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = make_pipeline(ExtractMiddlePart(), InfersentFeature(infersent), XGBClassifier(n_jobs=-1, n_estimators=1000))\n",
    "fitted = pl.fit(train, train['most_frequent_label'].values)\n",
    "predicted = fitted.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BETTER      0.768     0.751     0.759       273\n",
      "       WORSE      0.553     0.353     0.431       119\n",
      "        NONE      0.901     0.943     0.921      1048\n",
      "\n",
      "    accuracy                          0.858      1440\n",
      "   macro avg      0.740     0.682     0.704      1440\n",
      "weighted avg      0.847     0.858     0.850      1440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test['most_frequent_label'].values, predicted, labels=['BETTER', 'WORSE', 'NONE'], digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded\n"
     ]
    }
   ],
   "source": [
    "elmo = initialize_elmo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary test to define which version of ELMo is more suitable for our task (Original or Original5.5B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = make_pipeline(ExtractRawSentence(), ElmoFeature(elmo, batch_size), XGBClassifier(n_jobs=-1, n_estimators=1000))\n",
    "fitted = pl.fit(train, train['most_frequent_label'].values)\n",
    "predicted = fitted.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original (5.5B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BETTER      0.622     0.495     0.551       273\n",
      "       WORSE      0.609     0.118     0.197       119\n",
      "        NONE      0.823     0.943     0.879      1048\n",
      "\n",
      "    accuracy                          0.790      1440\n",
      "   macro avg      0.685     0.518     0.542      1440\n",
      "weighted avg      0.767     0.790     0.760      1440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test['most_frequent_label'].values, predicted, labels=['BETTER', 'WORSE', 'NONE'], digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BETTER      0.597     0.473     0.528       273\n",
      "       WORSE      0.400     0.067     0.115       119\n",
      "        NONE      0.822     0.945     0.879      1048\n",
      "\n",
      "    accuracy                          0.783      1440\n",
      "   macro avg      0.606     0.495     0.507      1440\n",
      "weighted avg      0.745     0.783     0.749      1440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test['most_frequent_label'].values, predicted, labels=['BETTER', 'WORSE', 'NONE'], digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Middle part of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = make_pipeline(ExtractMiddlePart(), ElmoFeature(elmo, batch_size), XGBClassifier(n_jobs=-1, n_estimators=1000))\n",
    "fitted = pl.fit(train, train['most_frequent_label'].values)\n",
    "predicted = fitted.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original (5.5B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BETTER      0.715     0.718     0.717       273\n",
      "       WORSE      0.578     0.218     0.317       119\n",
      "        NONE      0.888     0.949     0.917      1048\n",
      "\n",
      "    accuracy                          0.845      1440\n",
      "   macro avg      0.727     0.629     0.650      1440\n",
      "weighted avg      0.829     0.845     0.830      1440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test['most_frequent_label'].values, predicted, labels=['BETTER', 'WORSE', 'NONE'], digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BETTER      0.736     0.685     0.710       273\n",
      "       WORSE      0.558     0.244     0.339       119\n",
      "        NONE      0.880     0.952     0.915      1048\n",
      "\n",
      "    accuracy                          0.843      1440\n",
      "   macro avg      0.725     0.627     0.655      1440\n",
      "weighted avg      0.826     0.843     0.828      1440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test['most_frequent_label'].values, predicted, labels=['BETTER', 'WORSE', 'NONE'], digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating ELMo embeddings of sentences for testing different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = make_pipeline(ExtractRawSentence(), ElmoFeature(elmo, batch_size))\n",
    "pl = pl.fit(train, train['most_frequent_label'].values)\n",
    "full_elmo_embs_train = pl.transform(train)\n",
    "full_elmo_embs_test = pl.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"elmo/full_train_features.pkl\", \"wb\") as f:\n",
    "    pickle.dump(full_elmo_embs_train, f)\n",
    "with open(\"elmo/full_test_features.pkl\", \"wb\") as f:\n",
    "    pickle.dump(full_elmo_embs_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = make_pipeline(ExtractMiddlePart(), ElmoFeature(elmo, batch_size))\n",
    "pl = pl.fit(train, train['most_frequent_label'].values)\n",
    "mid_elmo_embs_train = pl.transform(train)\n",
    "mid_elmo_embs_test = pl.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"elmo/mid_train_features.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mid_elmo_embs_train, f)\n",
    "with open(\"elmo/mid_test_features.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mid_elmo_embs_test, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests with different classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results using only the middle part of the sentence are a lot better than the results with the use of the full sentences, so in further tests we will be using them.  \n",
    "Also, the version of ELMo is Original (5.5B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  22 tasks      | elapsed:   10.4s\n",
      "[Parallel(n_jobs=5)]: Done  70 out of  70 | elapsed:  8.6min finished\n",
      "/home/nick/.local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression scored on CV 0.852 for {'C': 0.1, 'penalty': 'l2'}\n",
      "Classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BETTER      0.746     0.766     0.756       273\n",
      "       WORSE      0.600     0.328     0.424       119\n",
      "        NONE      0.906     0.947     0.926      1048\n",
      "\n",
      "    accuracy                          0.861      1440\n",
      "   macro avg      0.751     0.680     0.702      1440\n",
      "weighted avg      0.850     0.861     0.852      1440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(solver='liblinear')\n",
    "params = {\n",
    "    'C': [10**i for i in range(-3, 4)],\n",
    "    'penalty' : ['l1', 'l2']\n",
    "}\n",
    "gs = GridSearchCV(clf, params, cv=5, n_jobs=-1, scoring='f1_micro', verbose=1)\n",
    "gs.fit(mid_elmo_embs_train, train['most_frequent_label'].values)\n",
    "\n",
    "name = clf.__class__.__name__\n",
    "f1_CV = gs.best_score_\n",
    "predicted = gs.predict(mid_elmo_embs_test)\n",
    "print(f'{name} scored on CV {round(f1_CV, 3)} for {gs.best_params_}')\n",
    "print('Classification_report')\n",
    "print(classification_report(test['most_frequent_label'].values, predicted, labels=['BETTER', 'WORSE', 'NONE'], digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=5)]: Done  50 out of  50 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier scored on CV 0.816 for {'metric': 'euclidean', 'n_neighbors': 10}\n",
      "Classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BETTER      0.617     0.707     0.659       273\n",
      "       WORSE      0.429     0.076     0.129       119\n",
      "        NONE      0.886     0.935     0.910      1048\n",
      "\n",
      "    accuracy                          0.821      1440\n",
      "   macro avg      0.644     0.573     0.566      1440\n",
      "weighted avg      0.797     0.821     0.798      1440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = KNeighborsClassifier()\n",
    "params = {\n",
    "    'n_neighbors': [3, 5, 10, 50, 100],\n",
    "    'metric' : ['euclidean', 'minkowski']\n",
    "}\n",
    "gs = GridSearchCV(clf, params, cv=5, n_jobs=-1, scoring='f1_micro', verbose=1)\n",
    "gs.fit(mid_elmo_embs_train, train['most_frequent_label'].values)\n",
    "\n",
    "name = clf.__class__.__name__\n",
    "f1_CV = gs.best_score_\n",
    "predicted = gs.predict(mid_elmo_embs_test)\n",
    "print(f'{name} scored on CV {round(f1_CV, 3)} for {gs.best_params_}')\n",
    "print('Classification_report')\n",
    "print(classification_report(test['most_frequent_label'].values, predicted, labels=['BETTER', 'WORSE', 'NONE'], digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=5)]: Done  60 out of  60 | elapsed:  8.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier scored on CV 0.829 for {'max_depth': 30, 'n_estimators': 1000}\n",
      "Classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BETTER      0.719     0.637     0.676       273\n",
      "       WORSE      1.000     0.008     0.017       119\n",
      "        NONE      0.852     0.973     0.909      1048\n",
      "\n",
      "    accuracy                          0.830      1440\n",
      "   macro avg      0.857     0.540     0.534      1440\n",
      "weighted avg      0.839     0.830     0.791      1440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier()\n",
    "params = {\n",
    "    'n_estimators': [10, 100, 1000],\n",
    "    'max_depth' : [3, 6, 10, 30]\n",
    "}\n",
    "gs = GridSearchCV(clf, params, cv=5, n_jobs=-1, scoring='f1_micro', verbose=1)\n",
    "gs.fit(mid_elmo_embs_train, train['most_frequent_label'].values)\n",
    "\n",
    "name = clf.__class__.__name__\n",
    "f1_CV = gs.best_score_\n",
    "predicted = gs.predict(mid_elmo_embs_test)\n",
    "print(f'{name} scored on CV {round(f1_CV, 3)} for {gs.best_params_}')\n",
    "print('Classification_report')\n",
    "print(classification_report(test['most_frequent_label'].values, predicted, labels=['BETTER', 'WORSE', 'NONE'], digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=5)]: Done  70 out of  70 | elapsed:  5.5min finished\n",
      "/home/nick/.local/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC scored on CV 0.858 for {'C': 10, 'kernel': 'rbf'}\n",
      "Classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BETTER      0.751     0.740     0.745       273\n",
      "       WORSE      0.617     0.244     0.349       119\n",
      "        NONE      0.892     0.957     0.924      1048\n",
      "\n",
      "    accuracy                          0.857      1440\n",
      "   macro avg      0.753     0.647     0.673      1440\n",
      "weighted avg      0.843     0.857     0.842      1440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = SVC()\n",
    "params = {\n",
    "    'C': [10**i for i in range(-3, 4)],\n",
    "    'kernel' : ['linear', 'rbf']\n",
    "}\n",
    "gs = GridSearchCV(clf, params, cv=5, n_jobs=-1, scoring='f1_micro', verbose=1)\n",
    "gs.fit(mid_elmo_embs_train, train['most_frequent_label'].values)\n",
    "\n",
    "name = clf.__class__.__name__\n",
    "f1_CV = gs.best_score_\n",
    "predicted = gs.predict(mid_elmo_embs_test)\n",
    "print(f'{name} scored on CV {round(f1_CV, 3)} for {gs.best_params_}')\n",
    "print('Classification_report')\n",
    "print(classification_report(test['most_frequent_label'].values, predicted, labels=['BETTER', 'WORSE', 'NONE'], digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = XGBClassifier()\n",
    "params = {\n",
    "    'n_estimators': [10, 100, 1000],\n",
    "    'max_depth' : [3, 6, 10]\n",
    "}\n",
    "gs = GridSearchCV(clf, params, cv=5, n_jobs=-1, scoring='f1_micro', verbose=1)\n",
    "gs.fit(mid_elmo_embs_train, train['most_frequent_label'].values)\n",
    "\n",
    "name = clf.__class__.__name__\n",
    "f1_CV = gs.best_score_\n",
    "predicted = gs.predict(mid_elmo_embs_test)\n",
    "print(f'{name} scored on CV {round(f1_CV, 3)} for {gs.best_params_}')\n",
    "print('Classification_report')\n",
    "print(classification_report(test['most_frequent_label'].values, predicted, labels=['BETTER', 'WORSE', 'NONE'], digits=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
